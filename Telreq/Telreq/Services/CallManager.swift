import Foundation
import AVFoundation
import CallKit
import Contacts
#if canImport(UIKit)
import UIKit
#endif
import os.log

/// é€šè©±ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹
/// 
/// AVAudioSessionã®è¨­å®šã€é€šè©±çŠ¶æ…‹ã®ç›£è¦–ã€è‡ªå‹•éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹/åœæ­¢ã‚’ç®¡ç†ã—ã¾ã™ã€‚
/// CallKitã¨ã®é€£æºã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ é€šè©±ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç›£è¦–ã—ã€é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§
/// éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã¨éŸ³å£°èªè­˜ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚
final class CallManager: NSObject, CallManagerProtocol, ObservableObject {
    
    // MARK: - Properties
    
    /// é€šè©±çŠ¶æ…‹ã®ç›£è¦–ãƒ‡ãƒªã‚²ãƒ¼ãƒˆ
    weak var delegate: CallManagerDelegate?
    
    /// ç¾åœ¨ã®é€šè©±çŠ¶æ…‹
    @Published private(set) var callState: CallState = .idle
    
    /// éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚µãƒ¼ãƒ“ã‚¹
    private let audioCaptureService: AudioCaptureService
    
    /// éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹
    private let speechRecognitionService: SpeechRecognitionService
    
    /// ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹
    private let textProcessingService: TextProcessingServiceProtocol
    
    /// ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹
    private let storageService: StorageServiceProtocol
    
    /// ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    private let offlineDataManager: OfflineDataManagerProtocol
    
    /// CallKité–¢é€£ (iOS ã®ã¿)
    #if canImport(CallKit) && !os(macOS)
    private let callObserver = CXCallObserver()
    #endif
    
    /// éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ (iOS ã®ã¿)
    #if canImport(AVFoundation) && !os(macOS)
    private let audioSession = AVAudioSession.sharedInstance()
    #endif
    
    /// ç¾åœ¨ã®é€šè©±æƒ…å ±
    private(set) var currentCall: CallInfo?
    
    /// å‡¦ç†çŠ¶æ…‹ãƒ•ãƒ©ã‚°ï¼ˆé‡è¤‡å‡¦ç†é˜²æ­¢ï¼‰
    private var isProcessingResult = false
    private let processingQueue = DispatchQueue(label: "com.telreq.processing", qos: .userInitiated)
    
    /// é€šè©±é–‹å§‹æ™‚åˆ»
    private var callStartTime: Date?
    
    /// è‡ªå‹•ã‚­ãƒ£ãƒ—ãƒãƒ£è¨­å®š
    private var autoCapturingEnabled: Bool = true
    
    /// ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãƒ•ã‚©ãƒ³è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆè¨­å®š
    private var autoSpeakerEnabled: Bool = true
    
    /// ãƒ­ã‚°å‡ºåŠ›ç”¨
    private let logger = Logger(subsystem: "com.telreq.app", category: "CallManager")
    
    /// ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ç”¨ã‚¿ã‚¹ã‚¯
    #if canImport(UIKit) && !os(macOS)
    private var backgroundTask: UIBackgroundTaskIdentifier = .invalid
    #endif
    
    /// éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›£è¦–ã‚¿ã‚¤ãƒãƒ¼
    private var sessionMonitorTimer: Timer?
    
    // MARK: - Initialization
    
    /// CallManagerã®åˆæœŸåŒ–
    /// - Parameters:
    ///   - audioCaptureService: éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚µãƒ¼ãƒ“ã‚¹
    ///   - speechRecognitionService: éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹
    ///   - textProcessingService: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹
    ///   - storageService: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹
    ///   - offlineDataManager: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    init(
        audioCaptureService: AudioCaptureService,
        speechRecognitionService: SpeechRecognitionService,
        textProcessingService: TextProcessingServiceProtocol,
        storageService: StorageServiceProtocol,
        offlineDataManager: OfflineDataManagerProtocol
    ) {
        self.audioCaptureService = audioCaptureService
        self.speechRecognitionService = speechRecognitionService
        self.textProcessingService = textProcessingService
        self.storageService = storageService
        self.offlineDataManager = offlineDataManager
        
        super.init()
        
        setupCallObserver()
        setupAudioSessionNotifications()
        setupServices()
        
        logger.info("CallManager initialized")
    }
    
    deinit {
        logger.info("CallManager deinitializing")
        stopMonitoring()
        cleanupServices()
        
        // é€šçŸ¥ã®ç›£è¦–ã‚’åœæ­¢
        NotificationCenter.default.removeObserver(self)
        
        logger.info("CallManager deinitialized")
    }
    
    // MARK: - Service Setup
    
    // MARK: - Public Methods
    
    /// é€šè©±ç›£è¦–ã‚’é–‹å§‹
    func startMonitoring() {
        logger.info("Starting call monitoring")
        
        #if canImport(CallKit) && !os(macOS)
        callObserver.setDelegate(self, queue: DispatchQueue.main)
        #endif
        startSessionMonitoring()
        
        logger.info("Call monitoring started")
    }
    
    /// é€šè©±ç›£è¦–ã‚’åœæ­¢
    func stopMonitoring() {
        logger.info("Stopping call monitoring")
        
        #if canImport(CallKit) && !os(macOS)
        callObserver.setDelegate(nil, queue: nil)
        #endif
        stopSessionMonitoring()
        stopCurrentCall()
        
        logger.info("Call monitoring stopped")
    }
    
    /// æ‰‹å‹•ã§éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’é–‹å§‹
    /// - Returns: é–‹å§‹æˆåŠŸãƒ•ãƒ©ã‚°
    @discardableResult
    func startAudioCapture() async -> Bool {
        logger.info("Manually starting audio capture")
        
        do {
            // æ‰‹å‹•éŒ²éŸ³ã®ãŸã‚ã®é€šè©±æƒ…å ±ã‚’ä½œæˆï¼ˆå…ˆã«ä½œæˆï¼‰
            if currentCall == nil {
                currentCall = CallInfo(
                    id: UUID(),
                    direction: .outgoing,
                    phoneNumber: "Manual Recording",
                    startTime: Date(),
                    isConnected: true
                )
                callStartTime = Date()
            }
            
            // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
            let success = try await audioCaptureService.startCapture()
            if success {
                // å°‘ã—é…å»¶ã•ã›ã¦ã‹ã‚‰éŸ³å£°èªè­˜ã‚’é–‹å§‹ï¼ˆå®‰å®šåŒ–ã®ãŸã‚ï¼‰
                try await Task.sleep(nanoseconds: 500_000_000) // 0.5ç§’
                
                do {
                    try await speechRecognitionService.startRealtimeRecognition()
                    logger.info("Manual audio capture and speech recognition started successfully")
                } catch {
                    logger.warning("Speech recognition failed to start, but audio capture is running: \(error.localizedDescription)")
                    // éŸ³å£°èªè­˜ãŒå¤±æ•—ã—ã¦ã‚‚éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã¯ç¶™ç¶š
                }
            }
            return success
        } catch {
            // ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã«ã‚¨ãƒ©ãƒ¼å‡¦ç†
            
            // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            currentCall = nil
            callStartTime = nil
            
            return false
        }
    }
    
    /// æ‰‹å‹•ã§éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’åœæ­¢
    func stopAudioCapture() {
        logger.info("Manually stopping audio capture")
        
        audioCaptureService.stopCapture()
        speechRecognitionService.stopRecognition()
        
        logger.info("Manual audio capture stopped")
    }
    
    /// æ‰‹å‹•éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’åœæ­¢ã—ã¦çµæœã‚’ä¿å­˜
    func stopAudioCaptureAndSave() async {
        logger.info("Stopping manual audio capture and saving results")
        
        // ç¾åœ¨ã®é€šè©±æƒ…å ±ãŒãªã„å ´åˆã¯ä»®ã®æƒ…å ±ã‚’ä½œæˆ
        if currentCall == nil {
            currentCall = CallInfo(
                id: UUID(),
                direction: .outgoing,
                phoneNumber: "Manual Recording",
                startTime: callStartTime ?? Date().addingTimeInterval(-60), // 1åˆ†å‰ã¨ä»®å®š
                isConnected: true
            )
        }
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
        stopAudioCapture()
        
        // ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã¨ä¿å­˜ã‚’å®Ÿè¡Œ
        await processCallTextAfterEnd()
        
        // é€šè©±æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        currentCall = nil
        callStartTime = nil
        
        logger.info("Manual audio capture stopped and results saved")
    }
    
    /// è‡ªå‹•ã‚­ãƒ£ãƒ—ãƒãƒ£è¨­å®šã‚’å¤‰æ›´
    /// - Parameter enabled: æœ‰åŠ¹/ç„¡åŠ¹ãƒ•ãƒ©ã‚°
    func setAutoCapturing(_ enabled: Bool) {
        autoCapturingEnabled = enabled
        logger.info("Auto capturing set to: \(enabled)")
    }
    
    /// ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãƒ•ã‚©ãƒ³è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆè¨­å®šã‚’å¤‰æ›´
    /// - Parameter enabled: æœ‰åŠ¹/ç„¡åŠ¹ãƒ•ãƒ©ã‚°
    func setAutoSpeaker(_ enabled: Bool) {
        autoSpeakerEnabled = enabled
        logger.info("Auto speaker set to: \(enabled)")
    }
    
    /// ç¾åœ¨ã®é€šè©±ã‚’å¼·åˆ¶çµ‚äº†ï¼ˆç·Šæ€¥æ™‚ç”¨ï¼‰
    func forceEndCurrentCall() {
        logger.warning("Force ending current call")
        stopCurrentCall()
        updateCallState(.idle)
    }
    
    /// éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ‰‹å‹•ã§è¨­å®š
    func configureAudioSession() async throws {
        logger.info("Manually configuring audio session")
        try await setupAudioSessionForCall()
    }
    
    /// é€šè©±è¨˜éŒ²ã‚’é–‹å§‹
    func startCallRecording() async throws {
        logger.info("Starting call recording")
        
        do {
            // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
            try await setupAudioSessionForCall()
            
            // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
            let captureSuccess = try await audioCaptureService.startCapture()
            if !captureSuccess {
                throw AppError.audioPermissionDenied
            }
            
            // éŸ³å£°èªè­˜é–‹å§‹
            try await speechRecognitionService.startRealtimeRecognition()
            
            // é€šè©±çŠ¶æ…‹ã‚’æ›´æ–°
            updateCallState(.active)
            
            logger.info("Call recording started successfully")
        } catch {
            // ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã«ã‚¨ãƒ©ãƒ¼å‡¦ç†
            throw error
        }
    }
    
    /// é€šè©±è¨˜éŒ²ã‚’åœæ­¢
    func stopCallRecording() async throws -> SpeechRecognitionResult {
        logger.info("Stopping call recording")
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
        audioCaptureService.stopCapture()
        
        // æœ€çµ‚çš„ãªéŸ³å£°èªè­˜çµæœã‚’å–å¾—
        let result = try await speechRecognitionService.getFinalRecognitionResult()
        
        // éŸ³å£°èªè­˜åœæ­¢
        speechRecognitionService.stopRecognition()
        
        // é€šè©±çŠ¶æ…‹ã‚’æ›´æ–°
        updateCallState(.idle)
        
        // é€šè©±çµ‚äº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        await processCallTextAfterEnd()
        
        logger.info("Call recording stopped successfully")
        return result
    }
    
    /// ãƒ¯ãƒ³ãƒœã‚¿ãƒ³ã§é€šè©±è¨˜éŒ²ã‚’é–‹å§‹
    func startCallRecordingWithOneButton() async -> Bool {
        logger.info("Starting one-button call recording")
        
        do {
            // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
            try await setupAudioSessionForCall()
            
            // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹
            let captureSuccess = try await audioCaptureService.startCapture()
            if !captureSuccess {
                // ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã«ã‚¨ãƒ©ãƒ¼å‡¦ç†
                return false
            }
            
            // éŸ³å£°èªè­˜é–‹å§‹
            try await speechRecognitionService.startRealtimeRecognition()
            
            // é€šè©±çŠ¶æ…‹ã‚’æ›´æ–°
            updateCallState(.active)
            
            logger.info("One-button call recording started successfully")
            return true
        } catch {
            // ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã«ã‚¨ãƒ©ãƒ¼å‡¦ç†
            return false
        }
    }
    
    /// é€šè©±çµ‚äº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
    func processCallTextAfterEnd() async {
        logger.info("Processing call text after end")
        
        guard currentCall != nil else {
            logger.warning("No call info available for text processing")
            return
        }
        
        do {
            // è¨­å®šã‹ã‚‰transcriptionæ–¹æ³•ã‚’å–å¾—
            let selectedTranscriptionMethod = getSelectedTranscriptionMethod()
            logger.info("Using transcription method: \(selectedTranscriptionMethod.displayName)")
            
            // é¸æŠã•ã‚ŒãŸæ–¹æ³•ã«SpeechRecognitionServiceã‚’åˆ‡ã‚Šæ›¿ãˆ
            speechRecognitionService.switchTranscriptionMethod(selectedTranscriptionMethod)
            
            // éŸ³å£°èªè­˜çµæœã‚’å–å¾—
            let recognitionResult = try await speechRecognitionService.getFinalRecognitionResult()
            
            guard !recognitionResult.text.isEmpty else {
                logger.warning("No text recognized from call")
                return
            }
            
            // é€šè©±ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆé¸æŠã•ã‚ŒãŸè»¢å†™æ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
            let metadata = CallMetadata(
                callDirection: currentCall?.direction ?? .incoming,
                audioQuality: .good,
                transcriptionMethod: selectedTranscriptionMethod,
                language: "ja-JP",
                confidence: recognitionResult.confidence,
                startTime: currentCall?.startTime ?? Date(),
                endTime: Date(),
                deviceInfo: await createDeviceInfo(),
                networkInfo: NetworkInfo(
                    connectionType: .wifi
                )
            )
            
            // é€šè©±æ™‚é–“ã‚’è¨ˆç®—
            let callDuration = callStartTime != nil ? Date().timeIntervalSince(callStartTime!) : 0
            
            // ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã¨æ§‹é€ åŒ–
            let structuredData = try await textProcessingService.structureCallData(
                recognitionResult.text,
                metadata: metadata
            )
            
            // é€šè©±æ™‚é–“ã¨å‚åŠ è€…ç•ªå·ã‚’æ›´æ–°ã—ãŸStructuredCallDataã‚’ä½œæˆ
            let updatedStructuredData = StructuredCallData(
                id: structuredData.id,
                timestamp: structuredData.timestamp,
                duration: callDuration,
                participantNumber: currentCall?.phoneNumber ?? "Manual Recording",
                audioFileUrl: structuredData.audioFileUrl,
                transcriptionText: structuredData.transcriptionText,
                summary: structuredData.summary,
                metadata: structuredData.metadata,
                isShared: structuredData.isShared,
                sharedWith: structuredData.sharedWith
            )
            
            // ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆ1ãƒ¶æœˆä¿æŒï¼‰
            try await offlineDataManager.saveLocalData(updatedStructuredData)
            
            // Azureã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            let storageUrl = try await storageService.saveCallData(updatedStructuredData)
            
            // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Azureã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•å‰Šé™¤ã•ã‚Œã‚‹ï¼‰
            #if canImport(AVFoundation) && !os(macOS)
            if let audioFileUrl = getCurrentAudioFileURL() {
                let audioStorageUrl = try await storageService.uploadAudioFile(audioFileUrl, for: updatedStructuredData.id.uuidString)
                logger.info("Audio file uploaded to: \(audioStorageUrl)")
            }
            #endif
            
            logger.info("Call text processed and saved successfully: \(storageUrl), Duration: \(callDuration)s")
            
            // ãƒ‡ãƒªã‚²ãƒ¼ãƒˆã«é€šçŸ¥ - ç¾åœ¨ã®ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«åˆã‚ã›ã¦ä¿®æ­£
            // delegate?.callManager(didCompleteCallProcessing: updatedStructuredData, summary: updatedStructuredData.summary)
            
        } catch {
            logger.error("Failed to process call text: \(error.localizedDescription)")
            delegate?.callManager(didFailWithError: error)
        }
    }
    
    // MARK: - Private Methods
    
    /// MainActorã§DeviceInfoä½œæˆ
    @MainActor
    private func createDeviceInfo() -> DeviceInfo {
        #if canImport(UIKit) && !os(macOS)
        return DeviceInfo(
            deviceModel: UIDevice.current.model,
            systemVersion: UIDevice.current.systemVersion,
            appVersion: Bundle.main.infoDictionary?["CFBundleShortVersionString"] as? String ?? "1.0.0"
        )
        #else
        return DeviceInfo(
            deviceModel: "Mac",
            systemVersion: ProcessInfo.processInfo.operatingSystemVersionString,
            appVersion: Bundle.main.infoDictionary?["CFBundleShortVersionString"] as? String ?? "1.0.0"
        )
        #endif
    }
    
    /// è¨­å®šã‹ã‚‰transcriptionæ–¹æ³•ã‚’å–å¾—
    private func getSelectedTranscriptionMethod() -> TranscriptionMethod {
        guard let savedMethodString = UserDefaults.standard.string(forKey: "selectedTranscriptionMethod"),
              let method = TranscriptionMethod(rawValue: savedMethodString) else {
            // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯iOS Speech Frameworkï¼ˆiOSå„ªå…ˆã®æ–¹é‡ã«å¾“ã£ã¦ï¼‰
            return .iosSpeech
        }
        return method
    }
    
    /// CallObserverã‚’è¨­å®š
    private func setupCallObserver() {
        // CallKitã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯æ—¢ã«å®Ÿè¡Œæ™‚ã«è‡ªå‹•ã§è¡Œã‚ã‚Œã‚‹
        logger.info("Call observer setup completed")
    }
    
    /// éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³é€šçŸ¥ã‚’è¨­å®š
    private func setupAudioSessionNotifications() {
        #if canImport(AVFoundation) && !os(macOS)
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioSessionInterruption),
            name: AVAudioSession.interruptionNotification,
            object: audioSession
        )
        
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioSessionRouteChange),
            name: AVAudioSession.routeChangeNotification,
            object: audioSession
        )
        
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioSessionMediaServicesLost),
            name: AVAudioSession.mediaServicesWereLostNotification,
            object: audioSession
        )
        
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioSessionMediaServicesReset),
            name: AVAudioSession.mediaServicesWereResetNotification,
            object: audioSession
        )
        #endif
        
        logger.info("Audio session notifications setup completed")
    }
    
    /// ã‚µãƒ¼ãƒ“ã‚¹ã‚’è¨­å®š
    private func setupServices() {
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒªã‚²ãƒ¼ãƒˆè¨­å®šï¼ˆå¾ªç’°å‚ç…§ã‚’é¿ã‘ã‚‹ãŸã‚å¼±å‚ç…§ã‚’ç¢ºå®Ÿã«ä½¿ç”¨ï¼‰
        audioCaptureService.delegate = self
        logger.info("AudioCaptureService delegate set")
        
        // éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒªã‚²ãƒ¼ãƒˆè¨­å®šï¼ˆå¾ªç’°å‚ç…§ã‚’é¿ã‘ã‚‹ãŸã‚å¼±å‚ç…§ã‚’ç¢ºå®Ÿã«ä½¿ç”¨ï¼‰
        speechRecognitionService.delegate = self
        logger.info("SpeechRecognitionService delegate set")
        
        logger.info("Services setup completed")
    }
    
    /// ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    private func cleanupServices() {
        logger.info("Cleaning up services")
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’å®‰å…¨ã«åœæ­¢
        if callState == .active {
            stopAudioCapture()
        }
        
        // ãƒ‡ãƒªã‚²ãƒ¼ãƒˆã‚’æ˜ç¤ºçš„ã«nilã«è¨­å®š
        audioCaptureService.delegate = nil
        speechRecognitionService.delegate = nil
        
        logger.info("Services cleanup completed")
    }
    
    /// ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›£è¦–ã‚’é–‹å§‹
    private func startSessionMonitoring() {
        sessionMonitorTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { [weak self] _ in
            self?.monitorAudioSession()
        }
    }
    
    /// ã‚»ãƒƒã‚·ãƒ§ãƒ³ç›£è¦–ã‚’åœæ­¢
    private func stopSessionMonitoring() {
        sessionMonitorTimer?.invalidate()
        sessionMonitorTimer = nil
    }
    
    /// éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç›£è¦–
    private func monitorAudioSession() {
        // é€šè©±ä¸­ã®ã¿ç›£è¦–
        guard callState == .active else { return }
        
        #if canImport(AVFoundation) && !os(macOS)
        // éŸ³å£°ãƒ«ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        let currentRoute = audioSession.currentRoute
        let isUsingBuiltInSpeaker = currentRoute.outputs.contains { output in
            output.portType == .builtInSpeaker
        }
        
        // ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãƒ•ã‚©ãƒ³ãŒç„¡åŠ¹ã«ãªã£ãŸå ´åˆã®è­¦å‘Š
        if !isUsingBuiltInSpeaker && autoSpeakerEnabled {
            logger.warning("Built-in speaker not active during call - audio quality may be poor")
        }
        #endif
    }
    
    /// é€šè©±çŠ¶æ…‹ã‚’æ›´æ–° - SwiftUIå®‰å…¨ç‰ˆ
    private func updateCallState(_ newState: CallState) {
        let oldState = callState
        
        // @Published ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®æ›´æ–°ã¯å¿…ãšMainActorã§å®Ÿè¡Œ
        Task { @MainActor in
            self.callState = newState
            
            self.logger.info("Call state changed: \(oldState) -> \(newState)")
            
            // çŠ¶æ…‹ã«å¿œã˜ãŸå‡¦ç†ã‚‚MainActorã§å®‰å…¨ã«å®Ÿè¡Œ
            switch newState {
            case .active:
                await self.handleCallStartedSafely()
            case .idle:
                await self.handleCallEndedSafely()
            case .connecting:
                await self.handleCallConnectingSafely()
            }
        }
    }
    
    /// é€šè©±é–‹å§‹å‡¦ç†
    private func handleCallStarted() {
        logger.info("Handling call started")
        
        callStartTime = Date()
        
        Task {
            do {
                // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
                try await setupAudioSessionForCall()
                
                // è‡ªå‹•ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒæœ‰åŠ¹ãªå ´åˆ
                if autoCapturingEnabled {
                    // å°‘ã—é…å»¶ã•ã›ã¦ã‹ã‚‰é–‹å§‹ï¼ˆé€šè©±å®‰å®šåŒ–ã®ãŸã‚ï¼‰
                    try await Task.sleep(nanoseconds: 2_000_000_000) // 2ç§’
                    await startAudioCapture()
                }
                
            } catch {
                logger.error("Failed to handle call started: \(error.localizedDescription)")
            }
        }
        
        // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯é–‹å§‹
        startBackgroundTask()
    }
    
    /// é€šè©±é–‹å§‹å‡¦ç† - SwiftUIå®‰å…¨ç‰ˆ
    @MainActor
    private func handleCallStartedSafely() async {
        logger.info("Handling call started safely")
        
        callStartTime = Date()
        
        do {
            // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
            try await setupAudioSessionForCall()
            
            // è‡ªå‹•ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒæœ‰åŠ¹ãªå ´åˆ
            if autoCapturingEnabled {
                // å°‘ã—é…å»¶ã•ã›ã¦ã‹ã‚‰é–‹å§‹ï¼ˆé€šè©±å®‰å®šåŒ–ã®ãŸã‚ï¼‰
                try await Task.sleep(nanoseconds: 2_000_000_000) // 2ç§’
                await startAudioCapture()
            }
            
        } catch {
            logger.error("Failed to handle call started: \(error.localizedDescription)")
        }
        
        // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯é–‹å§‹
        startBackgroundTask()
    }
    
    /// é€šè©±çµ‚äº†å‡¦ç† - SwiftUIå®‰å…¨ç‰ˆ
    @MainActor
    private func handleCallEndedSafely() async {
        logger.info("Handling call ended safely")
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
        stopAudioCapture()
        
        // é€šè©±çµ‚äº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        await processCallTextAfterEnd()
        
        // é€šè©±æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        currentCall = nil
        callStartTime = nil
        
        // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯çµ‚äº†
        endBackgroundTask()
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
        #if canImport(AVFoundation) && !os(macOS)
        do {
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
            logger.info("Audio session deactivated")
        } catch {
            logger.error("Failed to deactivate audio session: \(error.localizedDescription)")
        }
        #endif
    }
    
    /// é€šè©±æ¥ç¶šä¸­å‡¦ç† - SwiftUIå®‰å…¨ç‰ˆ
    @MainActor
    private func handleCallConnectingSafely() async {
        logger.info("Handling call connecting safely")
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æº–å‚™
        do {
            try await setupAudioSessionForCall()
        } catch {
            logger.error("Failed to setup audio session during connection: \(error.localizedDescription)")
        }
    }
    
    /// é€šè©±çµ‚äº†å‡¦ç†
    private func handleCallEnded() {
        logger.info("Handling call ended")
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
        stopAudioCapture()
        
        // é€šè©±çµ‚äº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        Task {
            await processCallTextAfterEnd()
        }
        
        // é€šè©±æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        currentCall = nil
        callStartTime = nil
        
        // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯çµ‚äº†
        endBackgroundTask()
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
        Task {
            #if canImport(AVFoundation) && !os(macOS)
            do {
                try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
                logger.info("Audio session deactivated")
            } catch {
                logger.error("Failed to deactivate audio session: \(error.localizedDescription)")
            }
            #endif
        }
    }
    
    /// é€šè©±æ¥ç¶šä¸­å‡¦ç†
    private func handleCallConnecting() {
        logger.info("Handling call connecting")
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æº–å‚™
        Task {
            do {
                try await setupAudioSessionForCall()
            } catch {
                logger.error("Failed to setup audio session during connection: \(error.localizedDescription)")
            }
        }
    }
    
    /// é€šè©±ç”¨éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®š
    private func setupAudioSessionForCall() async throws {
        logger.info("Setting up audio session for call")
        
        #if canImport(AVFoundation) && !os(macOS)
        try audioSession.setCategory(
            .playAndRecord,
            mode: .voiceChat,
            options: [.defaultToSpeaker, .allowBluetooth, .allowBluetoothA2DP, .mixWithOthers]
        )
        
        // éŸ³è³ªè¨­å®š
        try audioSession.setPreferredSampleRate(16000)
        try audioSession.setPreferredIOBufferDuration(0.023)
        
        // ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ‰åŠ¹åŒ–
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        // ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãƒ•ã‚©ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆè‡ªå‹•è¨­å®šãŒæœ‰åŠ¹ãªå ´åˆï¼‰
        if autoSpeakerEnabled {
            try audioSession.overrideOutputAudioPort(.speaker)
            logger.info("Switched to speaker output")
        }
        #endif
        
        logger.info("Audio session configured for call")
    }
    
    /// ç¾åœ¨ã®é€šè©±ã‚’åœæ­¢
    private func stopCurrentCall() {
        logger.info("Stopping current call")
        
        // éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£åœæ­¢
        stopAudioCapture()
        
        // é€šè©±çµ‚äº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        Task {
            await processCallTextAfterEnd()
        }
        
        // é€šè©±æƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        currentCall = nil
        callStartTime = nil
    }
    
    /// ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
    private func startBackgroundTask() {
        endBackgroundTask() // æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ã‚’çµ‚äº†
        
        #if canImport(UIKit) && !os(macOS)
        backgroundTask = UIApplication.shared.beginBackgroundTask(withName: "CallRecording") { [weak self] in
            self?.endBackgroundTask()
        }
        
        logger.info("Background task started: \(self.backgroundTask.rawValue)")
        #endif
    }
    
    /// ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’çµ‚äº†
    private func endBackgroundTask() {
        #if canImport(UIKit) && !os(macOS)
        if self.backgroundTask != .invalid {
            UIApplication.shared.endBackgroundTask(self.backgroundTask)
            self.backgroundTask = .invalid
            logger.info("Background task ended")
        }
        #endif
    }
    
    /// é€šè©±æƒ…å ±ã‚’ä½œæˆ
    #if canImport(CallKit) && !os(macOS)
    private func createCallInfo(from call: CXCall) -> CallInfo {
        let direction: CallDirection = call.isOutgoing ? .outgoing : .incoming
        let phoneNumber = call.uuid.uuidString
        
        return CallInfo(
            id: call.uuid,
            direction: direction,
            phoneNumber: phoneNumber,
            startTime: Date(),
            isConnected: call.hasConnected
        )
    }
    #endif
}

// MARK: - CXCallObserverDelegate

#if canImport(CallKit) && !os(macOS)
extension CallManager: CXCallObserverDelegate {
    
    func callObserver(_ callObserver: CXCallObserver, callChanged call: CXCall) {
        logger.info("Call changed: \(call.uuid), hasConnected: \(call.hasConnected), hasEnded: \(call.hasEnded), isOutgoing: \(call.isOutgoing)")
        
        if call.hasEnded {
            // é€šè©±çµ‚äº†
            if currentCall?.id == call.uuid {
                updateCallState(.idle)
            }
        } else if call.hasConnected {
            // é€šè©±é–‹å§‹
            currentCall = createCallInfo(from: call)
            updateCallState(.active)
        } else {
            // é€šè©±æ¥ç¶šä¸­
            currentCall = createCallInfo(from: call)
            updateCallState(.connecting)
        }
    }
}
#endif



// MARK: - Audio Session Notifications

extension CallManager {
    
    #if canImport(AVFoundation) && !os(macOS)
    @objc private func handleAudioSessionInterruption(_ notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        logger.info("Audio session interruption: \(type.rawValue)")
        
        switch type {
        case .began:
            logger.info("Audio session interrupted")
            // é€šè©±ä¸­ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆCallKitãŒå‡¦ç†ï¼‰
            
        case .ended:
            guard let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt else {
                return
            }
            
            let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
            if options.contains(.shouldResume) && callState == .active {
                logger.info("Resuming audio session after interruption")
                Task {
                    do {
                        try await setupAudioSessionForCall()
                    } catch {
                        logger.error("Failed to resume audio session: \(error.localizedDescription)")
                    }
                }
            }
            
        @unknown default:
            logger.warning("Unknown audio session interruption type")
        }
    }
    
    @objc private func handleAudioSessionRouteChange(_ notification: Notification) {
        guard let userInfo = notification.userInfo,
              let reasonValue = userInfo[AVAudioSessionRouteChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else {
            return
        }
        
        logger.info("Audio route changed: \(reason.rawValue)")
        
        switch reason {
        case .newDeviceAvailable:
            logger.info("New audio device available")
            
        case .oldDeviceUnavailable:
            logger.info("Audio device unavailable")
            
        case .categoryChange:
            logger.info("Audio category changed")
            
        case .override:
            logger.info("Audio route override")
            
        default:
            break
        }
        
        // é€šè©±ä¸­ã®å ´åˆã¯é©åˆ‡ãªå‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºä¿
        if callState == .active && autoSpeakerEnabled {
            Task {
                #if canImport(AVFoundation) && !os(macOS)
                do {
                    try audioSession.overrideOutputAudioPort(.speaker)
                } catch {
                    logger.error("Failed to override audio port: \(error.localizedDescription)")
                }
                #endif
            }
        }
    }
    
    @objc private func handleAudioSessionMediaServicesLost(_ notification: Notification) {
        logger.warning("Audio media services lost")
        stopAudioCapture()
    }
    
    @objc private func handleAudioSessionMediaServicesReset(_ notification: Notification) {
        logger.info("Audio media services reset")
        
        if callState == .active {
            Task {
                do {
                    try await setupAudioSessionForCall()
                    if autoCapturingEnabled {
                        await startAudioCapture()
                    }
                } catch {
                    logger.error("Failed to reconfigure after media services reset: \(error.localizedDescription)")
                }
            }
        }
    }
    
    /// ç¾åœ¨ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URLã‚’å–å¾—
    private func getCurrentAudioFileURL() -> URL? {
        // ç¾åœ¨ã®å®Ÿè£…ã§ã¯ã€AudioCaptureServiceã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URLã‚’å–å¾—ã™ã‚‹æ–¹æ³•ãŒãªã„ãŸã‚ã€
        // ä¸€æ™‚çš„ãªå®Ÿè£…ã¨ã—ã¦ã€Documentsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        let documentsDirectory = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
        let audioDirectory = documentsDirectory.appendingPathComponent("AudioFiles")
        
        do {
            let files = try FileManager.default.contentsOfDirectory(at: audioDirectory, includingPropertiesForKeys: nil)
            let audioFiles = files.filter { $0.pathExtension == "m4a" || $0.pathExtension == "wav" }
            
            // æœ€æ–°ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
            return audioFiles.sorted { $0.lastPathComponent > $1.lastPathComponent }.first
        } catch {
            logger.warning("No audio files found: \(error.localizedDescription)")
            return nil
        }
    }
    #endif
}

// MARK: - AudioCaptureDelegate Implementation

extension CallManager: AudioCaptureDelegate {
    func audioCaptureDidStart() {
        logger.info("Audio capture started - beginning speech recognition data accumulation")
    }
    
    func audioCaptureDidStop() {
        logger.info("Audio capture stopped - will process accumulated speech data")
        
        // éŒ²éŸ³åœæ­¢æ™‚ã«STTå‡¦ç†ã‚’å®Ÿè¡Œ
        Task {
            await processRecordedAudio()
        }
    }
    
    /// éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    private func processRecordedAudio() async {
        logger.info("Starting speech recognition processing of recorded audio")
        
        do {
            // SpeechRecognitionServiceã§æœ€çµ‚çš„ãªéŸ³å£°èªè­˜çµæœã‚’å–å¾—
            let result = try await speechRecognitionService.getFinalRecognitionResult()
            logger.info("Speech recognition successful: \(result.text.prefix(50))...")
            
            // çµæœã‚’ãƒ‡ãƒªã‚²ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã—ã¦å‡¦ç†ï¼ˆé‡è¤‡é˜²æ­¢æ¸ˆã¿ï¼‰
            await handleSpeechRecognitionResult(result)
            
        } catch {
            logger.error("Speech recognition processing failed: \(error.localizedDescription)")
            
            // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’ä½œæˆ
            let fallbackResult = SpeechRecognitionResult(
                text: "éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
                confidence: 0.0,
                method: .azureSpeech,
                language: "ja-JP",
                processingTime: 0,
                segments: []
            )
            
            await handleSpeechRecognitionResult(fallbackResult)
        }
    }
    
    func audioCapture(didReceiveBuffer buffer: AVAudioPCMBuffer) {
        // éŸ³å£°ãƒãƒƒãƒ•ã‚¡ã‚’SpeechRecognitionServiceã«é€ä¿¡ã—ã¦è“„ç©
        speechRecognitionService.accumulateAudioData(buffer)
    }
    
    func audioCapture(didUpdateLevel level: Float) {
        // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã®æ›´æ–°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    }
    
    func audioCapture(didEncounterError error: Error) {
        logger.error("Audio capture error: \(error.localizedDescription)")
    }
}

// MARK: - SpeechRecognitionDelegate Implementation

extension CallManager: SpeechRecognitionDelegate {
    func speechRecognition(didRecognizeText text: String, isFinal: Bool) {
        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã‚‹ãŸã‚ã€ä½•ã‚‚ã—ãªã„
        logger.info("Speech recognition text received (ignored): \(text.prefix(50))...")
    }
    
    func speechRecognition(didCompleteWithResult result: SpeechRecognitionResult) {
        logger.info("Speech recognition completed: \(result.text.prefix(50))...")
        // å®Œäº†ã—ãŸéŸ³å£°èªè­˜çµæœã‚’å‡¦ç†ï¼ˆé‡è¤‡é˜²æ­¢æ¸ˆã¿ï¼‰
        Task {
            await handleSpeechRecognitionResult(result)
        }
    }
    
    func speechRecognition(didFailWithError error: Error) {
        logger.error("Speech recognition failed: \(error.localizedDescription)")
    }
    
    func speechRecognitionDidTimeout() {
        logger.warning("Speech recognition timed out")
    }
    
    /// éŸ³å£°èªè­˜çµæœã‚’å‡¦ç† - SwiftUIå®‰å…¨ç‰ˆï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
    private func handleSpeechRecognitionResult(_ result: SpeechRecognitionResult) async {
        // é‡è¤‡å‡¦ç†ã‚’é˜²ããŸã‚ã®ãƒã‚§ãƒƒã‚¯
        return await withCheckedContinuation { continuation in
            processingQueue.async { [weak self] in
                guard let self = self, !self.isProcessingResult else {
                    continuation.resume()
                    return
                }
                
                self.isProcessingResult = true
                self.logger.info("ğŸ”„ Processing speech recognition result (length: \(result.text.count))")
                
                Task { [weak self] in
                    await self?.performSpeechProcessing(result)
                    
                    self?.processingQueue.async { [weak self] in
                        self?.isProcessingResult = false
                        continuation.resume()
                    }
                }
            }
        }
    }
    
    /// å®Ÿéš›ã®éŸ³å£°å‡¦ç†ã‚’å®Ÿè¡Œ
    private func performSpeechProcessing(_ result: SpeechRecognitionResult) async {
        do {
            // 1. AIè¦ç´„ã‚’å®Ÿè¡Œ - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
            logger.info("ğŸ“ Starting text summarization for text length: \(result.text.count)")
            let summary = try await AsyncDebugHelpers.shared.trackAsyncTask(
                {
                    try await self.textProcessingService.summarizeText(result.text)
                },
                name: "TextSummarization"
            )
            
            // 2. é€šè©±ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            let metadata = createCallMetadata()
            
            // 3. æ§‹é€ åŒ–ã•ã‚ŒãŸé€šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            let structuredData = try await textProcessingService.structureCallData(result.text, metadata: metadata)
            
            // 4. ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œï¼‰
            try await saveCallDataLocally(structuredData)
            
            // 5. ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ä¿å­˜ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            await saveCallDataToCloud(structuredData)
            
            // 6. UIã«çµæœã‚’é€šçŸ¥ï¼ˆãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—è¡¨ç¤ºç”¨ï¼‰
            await notifyCallProcessingComplete(structuredData, summary: summary)
            
            logger.info("Call processing completed successfully")
            
        } catch {
            logger.error("Failed to process call data: \(error.localizedDescription)")
            
            // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚åŸºæœ¬çš„ãªä¿å­˜ã¯å®Ÿè¡Œ
            let fallbackMetadata = createCallMetadata()
            let fallbackSummary = CallSummary(
                keyPoints: ["éŸ³å£°èªè­˜å®Œäº†"],
                summary: "éŸ³å£°èªè­˜ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€AIå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ",
                duration: 0,
                participants: ["Unknown"],
                actionItems: [],
                tags: [],
                confidence: result.confidence
            )
            
            let fallbackData = StructuredCallData(
                timestamp: Date(),
                duration: 0,
                participantNumber: currentCall?.phoneNumber ?? "Unknown",
                audioFileUrl: "",
                transcriptionText: result.text,
                summary: fallbackSummary,
                metadata: fallbackMetadata
            )
            
            do {
                try await saveCallDataLocally(fallbackData)
                await notifyCallProcessingComplete(fallbackData, summary: fallbackData.summary)
            } catch {
                logger.error("Failed to save fallback data: \(error.localizedDescription)")
            }
        }
    }
    
    /// é€šè©±ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    private func createCallMetadata() -> CallMetadata {
        let deviceInfo = DeviceInfo(
            deviceModel: "iOS Simulator",
            systemVersion: "18.5",
            appVersion: "1.0.0"
        )
        
        let networkInfo = NetworkInfo(
            connectionType: .wifi,
            signalStrength: 100
        )
        
        // è¨­å®šã‹ã‚‰è»¢å†™æ–¹æ³•ã‚’å–å¾—
        let selectedTranscriptionMethod = getSelectedTranscriptionMethod()
        
        return CallMetadata(
            callDirection: currentCall?.direction ?? .incoming,
            audioQuality: .good,
            transcriptionMethod: selectedTranscriptionMethod,
            language: "ja-JP",
            confidence: 0.8,
            startTime: callStartTime ?? Date(),
            endTime: Date(),
            deviceInfo: deviceInfo,
            networkInfo: networkInfo
        )
    }
    
    /// é€šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    private func saveCallDataLocally(_ data: StructuredCallData) async throws {
        logger.info("Saving local data for call ID: \(data.id)")
        try await offlineDataManager.saveLocalData(data)
        logger.info("Successfully saved call data locally")
    }
    
    /// é€šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¦ãƒ‰ã«ä¿å­˜
    private func saveCallDataToCloud(_ data: StructuredCallData) async {
        do {
            logger.info("Attempting to save call data to cloud storage")
            let _ = try await storageService.saveCallData(data)
            logger.info("Successfully saved call data to cloud")
        } catch {
            logger.warning("Failed to save to cloud storage: \(error.localizedDescription)")
            // ã‚¯ãƒ©ã‚¦ãƒ‰ä¿å­˜ã«å¤±æ•—ã—ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã¯ã—ãªã„ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œï¼‰
        }
    }
    
    /// é€šè©±å‡¦ç†å®Œäº†ã‚’UIã«é€šçŸ¥
    private func notifyCallProcessingComplete(_ data: StructuredCallData, summary: CallSummary) async {
        logger.info("Notifying UI of call processing completion")
        
        await MainActor.run {
            // ãƒ‡ãƒªã‚²ãƒ¼ãƒˆã«é€šçŸ¥ï¼ˆViewModelãŒå—ã‘å–ã‚‹ï¼‰
            delegate?.callManager(didCompleteCallProcessing: data, summary: summary)
        }
    }
}

// MARK: - Supporting Types

/// é€šè©±çŠ¶æ…‹
enum CallState: Equatable, CustomStringConvertible {
    case idle
    case connecting
    case active
    
    var description: String {
        switch self {
        case .idle:
            return "idle"
        case .connecting:
            return "connecting"
        case .active:
            return "active"
        }
    }
}

/// é€šè©±æƒ…å ±
struct CallInfo {
    let id: UUID
    let direction: CallDirection
    let phoneNumber: String
    let startTime: Date
    let isConnected: Bool
}


// MARK: - Debug Support

#if DEBUG
extension CallManager {
    
    /// ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
    func printDebugInfo() {
        #if canImport(AVFoundation) && !os(macOS)
        let backgroundTaskInfo = "- Background Task: \(self.backgroundTask.rawValue)"
        let audioSessionInfo = """
            - Audio Session Category: \(self.audioSession.category.rawValue)
            - Audio Session Mode: \(self.audioSession.mode.rawValue)
            """
        #else
        let backgroundTaskInfo = "- Background Task: Not available on macOS"
        let audioSessionInfo = "- Audio Session: Not available on macOS"
        #endif
        
        logger.debug("""
            CallManager Debug Info:
            - Call State: \(self.callState)
            - Current Call: \(self.currentCall?.phoneNumber ?? "none")
            - Auto Capturing: \(self.autoCapturingEnabled)
            - Auto Speaker: \(self.autoSpeakerEnabled)
            \(backgroundTaskInfo)
            \(audioSessionInfo)
            """)
    }
}
#endif